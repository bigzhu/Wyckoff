import requests
import zipfile
import io
import pandas as pd
import os
import numpy as np
from datetime import datetime, timedelta
from concurrent.futures import ThreadPoolExecutor, as_completed

# ================= é…ç½®åŒº =================
SYMBOL = "ADAUSDC"
INTERVAL = "1d"
START_DATE = datetime(2024, 1, 1) 
END_DATE = datetime.now() - timedelta(days=1)
MAX_WORKERS = 20
CACHE_DIR = "binance_data_cache"

BASE_URL = f"https://data.binance.vision/data/spot/daily/klines/{SYMBOL}/{INTERVAL}"
COLUMNS = [
    "Open_time", "Open", "High", "Low", "Close", "Volume",
    "Close_time", "Quote_asset_volume", "Number_of_trades",
    "Taker_buy_base_asset_volume", "Taker_buy_quote_asset_volume", "Ignore"
]
# ==========================================

if not os.path.exists(CACHE_DIR):
    os.makedirs(CACHE_DIR)

def get_data_for_day(date_str):
    file_name = f"{SYMBOL}-{INTERVAL}-{date_str}.zip"
    local_path = os.path.join(CACHE_DIR, file_name)
    
    # 1. å°è¯•æœ¬åœ°åŠ è½½
    if os.path.exists(local_path):
        try:
            with zipfile.ZipFile(local_path) as z:
                with z.open(z.namelist()[0]) as f:
                    # on_bad_lines='skip' é˜²æ­¢æŸè¡Œæ•°æ®åˆ—æ•°ä¸å¯¹
                    df = pd.read_csv(f, header=None, names=COLUMNS, dtype=str, on_bad_lines='skip')
                    return df, "Local"
        except:
            pass 

    # 2. æœ¬åœ°ä¸å­˜åœ¨åˆ™ä¸‹è½½
    url = f"{BASE_URL}/{file_name}"
    try:
        response = requests.get(url, timeout=10)
        if response.status_code == 200:
            with open(local_path, 'wb') as f_out:
                f_out.write(response.content)
            with zipfile.ZipFile(io.BytesIO(response.content)) as z:
                with z.open(z.namelist()[0]) as f:
                    return pd.read_csv(f, header=None, names=COLUMNS, dtype=str, on_bad_lines='skip'), "Download"
        return None, "Missing"
    except:
        return None, "Error"

def main():
    date_list = [ (START_DATE + timedelta(days=i)).strftime("%Y-%m-%d") 
                 for i in range((END_DATE - START_DATE).days + 1) ]

    all_dfs = []
    print(f"ğŸš€ å¯åŠ¨ | åˆå¹¶å¤„ç†ä¸­...")

    with ThreadPoolExecutor(max_workers=MAX_WORKERS) as executor:
        future_to_date = {executor.submit(get_data_for_day, d): d for d in date_list}
        for i, future in enumerate(as_completed(future_to_date), 1):
            df, source = future.result()
            if df is not None: all_dfs.append(df)
            print(f"\rè¿›åº¦: [{i}/{len(date_list)}] å¤„ç†æº: {source:<10}", end="", flush=True)

    if not all_dfs:
        print("\nâŒ æœªèƒ½è·å–æ•°æ®ã€‚")
        return

    print("\nğŸ“¥ æ­£åœ¨æ‰§è¡Œæ·±åº¦æ•°æ®æ¸…æ´—...")
    final_df = pd.concat(all_dfs, ignore_index=True)

    # --- å…³é”®æ¸…æ´—æ­¥éª¤ ---
    # 1. è½¬æ¢ä¸ºæ•°å­—ï¼Œéæ³•å­—ç¬¦å˜ NaN
    final_df['Open_time'] = pd.to_numeric(final_df['Open_time'], errors='coerce')
    
    # 2. æ ¸å¿ƒï¼šè¿‡æ»¤æ‰å¼‚å¸¸çš„æ—¶é—´æˆ³æ•°å€¼
    # æ­£å¸¸ 2024-2026 å¹´çš„æ¯«ç§’æ—¶é—´æˆ³åº”è¯¥åœ¨ 1.7e12 åˆ° 1.8e12 ä¹‹é—´
    # æˆ‘ä»¬è®¾å®šä¸€ä¸ªåˆç†çš„é˜ˆå€¼ï¼š1,500,000,000,000 åˆ° 2,000,000,000,000
    valid_mask = (final_df['Open_time'] > 1500000000000) & (final_df['Open_time'] < 2000000000000)
    final_df = final_df[valid_mask].copy()
    
    # 3. æ’åº
    final_df = final_df.sort_values('Open_time').drop_duplicates(subset=['Open_time'])

    # 4. å®‰å…¨è½¬æ¢æ—¥æœŸ
    try:
        # unit='ms' é…åˆå·²ç»è¿‡æ»¤è¿‡çš„æ•°å€¼ï¼Œç»ä¸ä¼šå†æŠ¥ OutOfBounds
        final_df['Human_Time'] = pd.to_datetime(final_df['Open_time'], unit='ms') + timedelta(hours=8)
        
        # è½¬æ¢ä»·æ ¼åˆ—ä¸ºæµ®ç‚¹æ•°ï¼Œæ–¹ä¾¿åç»­åˆ†æ
        price_cols = ['Open', 'High', 'Low', 'Close', 'Volume']
        for col in price_cols:
            final_df[col] = pd.to_numeric(final_df[col], errors='coerce')

        # æ•´ç†åˆ—é¡ºåº
        cols = ['Human_Time'] + [c for c in final_df.columns if c != 'Human_Time']
        final_df = final_df[cols]
    except Exception as e:
        print(f"\nâš ï¸ æ—¥æœŸè½¬æ¢è­¦å‘Š (å·²è·³è¿‡è½¬æ¢): {e}")

    output_name = f"{SYMBOL}_{INTERVAL}_Cleaned.csv"
    final_df.to_csv(output_name, index=False)
    print(f"\nâœ… æˆåŠŸï¼æ•°æ®å·²æ¸…æ´—ã€‚")
    print(f"ğŸ“Š æœ€ç»ˆè¡Œæ•°: {len(final_df)} | æ–‡ä»¶: {output_name}")

if __name__ == "__main__":
    main()